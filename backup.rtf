{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf100
{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red183\green111\blue179;\red23\green23\blue23;\red202\green202\blue202;
\red89\green138\blue67;\red70\green137\blue204;\red67\green192\blue160;\red194\green126\blue101;\red212\green214\blue154;
\red140\green211\blue254;\red167\green197\blue152;}
{\*\expandedcolortbl;;\cssrgb\c77255\c52549\c75294;\cssrgb\c11765\c11765\c11765;\cssrgb\c83137\c83137\c83137;
\cssrgb\c41569\c60000\c33333;\cssrgb\c33725\c61176\c83922;\cssrgb\c30588\c78824\c69020;\cssrgb\c80784\c56863\c47059;\cssrgb\c86275\c86275\c66667;
\cssrgb\c61176\c86275\c99608;\cssrgb\c70980\c80784\c65882;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl360\partightenfactor0

\f0\fs24 \cf2 \cb3 \expnd0\expndtw0\kerning0
from\cf4  keras.preprocessing.image \cf2 import\cf4  ImageDataGenerator\cb1 \
\cf2 \cb3 from\cf4  keras.models \cf2 import\cf4  Sequential, Model\cb1 \
\cf2 \cb3 from\cf4  keras.layers \cf2 import\cf4  InputLayer, Conv2D, UpSampling2D, Input, RepeatVector, Reshape, concatenate\cb1 \
\cf2 \cb3 from\cf4  keras.applications.inception_resnet_v2 \cf2 import\cf4  InceptionResNetV2, preprocess_input\cb1 \
\cf2 \cb3 from\cf4  skimage.io \cf2 import\cf4  imread, imsave\cb1 \
\cf2 \cb3 from\cf4  skimage.color \cf2 import\cf4  rgb2lab, lab2rgb, gray2rgb, rgb2gray\cb1 \
\cf2 \cb3 from\cf4  skimage.transform \cf2 import\cf4  resize\cb1 \
\cf2 \cb3 import\cf4  tensorflow \cf2 as\cf4  tf\cb1 \
\cf2 \cb3 import\cf4  numpy \cf2 as\cf4  np\cb1 \
\cf2 \cb3 import\cf4  os\cb1 \
\
\pard\pardeftab720\sl360\partightenfactor0
\cf5 \cb3 # use an encoder - decoder method \cf4 \cb1 \
\cf5 \cb3 # reference paper: https://arxiv.org/pdf/1712.03400.pdf\cf4 \cb1 \
\
\pard\pardeftab720\sl360\partightenfactor0
\cf6 \cb3 class\cf4  \cf7 InceptionModel\cf4 :\cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb3     \cf8 '''\cf4 \cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf8 \cb3     Full encoder - decoder + inception model\cf4 \cb1 \
\cf8 \cb3     '''\cf4 \cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb3     \cf6 def\cf4  \cf9 __init__\cf4 (\cf10 self\cf4 , \cf10 input_shape\cf4 ):\cb1 \
\cb3         \cf8 '''\cf4 \cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf8 \cb3         Uses Keras Functional API to create model\cf4 \cb1 \
\cf8 \cb3         '''\cf4 \cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb3         \cf5 # create encoder\cf4 \cb1 \
\cb3         encoder_inpt = Input(\cf10 shape\cf4 =(\cf11 256\cf4 , \cf11 256\cf4 , \cf11 1\cf4 , ))\cb1 \
\cb3         encoder = Conv2D(\cf11 64\cf4 , (\cf11 3\cf4 , \cf11 3\cf4 ), \cf10 strides\cf4 =\cf11 2\cf4 , \cf10 activation\cf4 =\cf8 'relu'\cf4 , \cf10 padding\cf4 =\cf8 'same'\cf4 )(encoder_inpt)\cb1 \
\cb3         encoder = Conv2D(\cf11 128\cf4 , (\cf11 3\cf4 , \cf11 3\cf4 ), \cf10 strides\cf4 =\cf11 1\cf4 , \cf10 activation\cf4 =\cf8 'relu'\cf4 , \cf10 padding\cf4 =\cf8 'same'\cf4 )(encoder)\cb1 \
\cb3         encoder = Conv2D(\cf11 128\cf4 , (\cf11 3\cf4 , \cf11 3\cf4 ), \cf10 strides\cf4 =\cf11 2\cf4 , \cf10 activation\cf4 =\cf8 'relu'\cf4 , \cf10 padding\cf4 =\cf8 'same'\cf4 )(encoder)\cb1 \
\cb3         encoder = Conv2D(\cf11 256\cf4 , (\cf11 3\cf4 , \cf11 3\cf4 ), \cf10 strides\cf4 =\cf11 1\cf4 , \cf10 activation\cf4 =\cf8 'relu'\cf4 , \cf10 padding\cf4 =\cf8 'same'\cf4 )(encoder)\cb1 \
\cb3         encoder = Conv2D(\cf11 256\cf4 , (\cf11 3\cf4 , \cf11 3\cf4 ), \cf10 strides\cf4 =\cf11 2\cf4 , \cf10 activation\cf4 =\cf8 'relu'\cf4 , \cf10 padding\cf4 =\cf8 'same'\cf4 )(encoder)\cb1 \
\cb3         encoder = Conv2D(\cf11 512\cf4 , (\cf11 3\cf4 , \cf11 3\cf4 ), \cf10 strides\cf4 =\cf11 1\cf4 , \cf10 activation\cf4 =\cf8 'relu'\cf4 , \cf10 padding\cf4 =\cf8 'same'\cf4 )(encoder)\cb1 \
\cb3         encoder = Conv2D(\cf11 512\cf4 , (\cf11 3\cf4 , \cf11 3\cf4 ), \cf10 strides\cf4 =\cf11 1\cf4 , \cf10 activation\cf4 =\cf8 'relu'\cf4 , \cf10 padding\cf4 =\cf8 'same'\cf4 )(encoder)\cb1 \
\cb3         encoder = Conv2D(\cf11 256\cf4 , (\cf11 3\cf4 , \cf11 3\cf4 ), \cf10 strides\cf4 =\cf11 1\cf4 , \cf10 activation\cf4 =\cf8 'relu'\cf4 , \cf10 padding\cf4 =\cf8 'same'\cf4 )(encoder)\cb1 \
\
\cb3         \cf5 # create inception embed layer\cf4 \cb1 \
\cb3         inception_inpt = Input(\cf10 shape\cf4 =(\cf11 1000\cf4 , ))\cb1 \
\
\cb3         \cf5 # fusion \cf4 \cb1 \
\cb3         fusion = RepeatVector((\cf11 256\cf4  * \cf11 256\cf4 )/(\cf11 8\cf4  ** \cf11 2\cf4 ))(inception_inpt)\cb1 \
\cb3         fusion = Reshape([\cf11 32\cf4 , \cf11 32\cf4 , \cf11 1000\cf4 ])(fusion)\cb1 \
\cb3         fusion = concatenate([encoder, fusion])\cb1 \
\cb3         fusion = Conv2D(\cf11 256\cf4 , (\cf11 1\cf4 , \cf11 1\cf4 ), \cf10 strides\cf4 =\cf11 1\cf4 , \cf10 activation\cf4 =\cf8 'relu'\cf4 , \cf10 padding\cf4 =\cf8 'same'\cf4 )(fusion)\cb1 \
\
\cb3         \cf5 # create decoder\cf4 \cb1 \
\cb3         decoder = Conv2D(\cf11 128\cf4 , (\cf11 3\cf4 , \cf11 3\cf4 ), \cf10 strides\cf4 =\cf11 1\cf4 , \cf10 activation\cf4 =\cf8 'relu'\cf4 , \cf10 padding\cf4 =\cf8 'same'\cf4 )(fusion)\cb1 \
\cb3         decoder = UpSampling2D((\cf11 2\cf4 , \cf11 2\cf4 ))(decoder)\cb1 \
\cb3         decoder = Conv2D(\cf11 64\cf4 , (\cf11 3\cf4 , \cf11 3\cf4 ), \cf10 strides\cf4 =\cf11 1\cf4 , \cf10 activation\cf4 =\cf8 'relu'\cf4 , \cf10 padding\cf4 =\cf8 'same'\cf4 )(decoder)\cb1 \
\cb3         decoder = Conv2D(\cf11 64\cf4 , (\cf11 3\cf4 , \cf11 3\cf4 ), \cf10 strides\cf4 =\cf11 1\cf4 , \cf10 activation\cf4 =\cf8 'relu'\cf4 , \cf10 padding\cf4 =\cf8 'same'\cf4 )(decoder)\cb1 \
\cb3         decoder = UpSampling2D((\cf11 2\cf4 , \cf11 2\cf4 ))(decoder)\cb1 \
\cb3         decoder = Conv2D(\cf11 32\cf4 , (\cf11 3\cf4 , \cf11 3\cf4 ), \cf10 strides\cf4 =\cf11 1\cf4 , \cf10 activation\cf4 =\cf8 'relu'\cf4 , \cf10 padding\cf4 =\cf8 'same'\cf4 )(decoder)\cb1 \
\cb3         decoder = Conv2D(\cf11 2\cf4 , (\cf11 3\cf4 , \cf11 3\cf4 ), \cf10 strides\cf4 =\cf11 1\cf4 , \cf10 activation\cf4 =\cf8 'relu'\cf4 , \cf10 padding\cf4 =\cf8 'same'\cf4 )(decoder)\cb1 \
\cb3         decoder = UpSampling2D((\cf11 2\cf4 , \cf11 2\cf4 ))(decoder)\cb1 \
\
\cb3         model = Model(\cf10 inputs\cf4 =[encoder, inception_inpt], \cf10 outputs\cf4 =[decoder])\cb1 \
\
\cb3         \cf6 self\cf4 .model = model\cb1 \
\
\cb3         \cf5 # inception_inpt = Input(shape=(299, 299, 1))\cf4 \cb1 \
\cb3         \cf5 # inception = self.load_inception_res_net()\cf4 \cb1 \
\
\cb3     \cf6 def\cf4  \cf9 load_inception_res_net\cf4 (\cf10 self\cf4 ):\cb1 \
\cb3         \cf8 '''\cf4 \cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf8 \cb3         Load inception resnet with pretrained weights for \cf4 \cb1 \
\cf8 \cb3         high level feature extraction\cf4 \cb1 \
\cf8 \cb3         '''\cf4 \cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb3         inception = InceptionResNetV2(\cf10 weights\cf4 =\cf6 None\cf4 , \cf10 include_top\cf4 =\cf6 True\cf4 )\cb1 \
\cb3         inception.load_weights(\cf8 'local/data/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5'\cf4 )\cb1 \
\cb3         inception.graph = tf.get_default_graph()\cb1 \
\cb3         \cf2 return\cf4  inception\cb1 \
\
\cb3     \cf6 def\cf4  \cf9 create_inception_embedding\cf4 (\cf10 self\cf4 , \cf10 gray\cf4 ):\cb1 \
\cb3         \cf8 '''\cf4 \cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf8 \cb3         Pass image through inception and obtain high level \cf4 \cb1 \
\cf8 \cb3         features before softmax\cf4 \cb1 \
\cf8 \cb3         '''\cf4 \cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb3         inception = \cf6 self\cf4 .load_inception_res_net()\cb1 \
\
\cb3         gray_resized = []\cb1 \
\cb3         \cf2 for\cf4  i \cf6 in\cf4  gray:\cb1 \
\cb3             i = resize(i, (\cf11 299\cf4 , \cf11 299\cf4 , \cf11 3\cf4 ), \cf10 mode\cf4 =\cf8 'constant'\cf4 )\cb1 \
\cb3             gray_resized.append(i)\cb1 \
\cb3         gray_resized = np.array(gray_resized)\cb1 \
\cb3         gray_resized = preprocess_input(gray_resized)\cb1 \
\cb3         \cf2 with\cf4  inception.graph.as_default():\cb1 \
\cb3             embed = inception.predict(gray_resized)\cb1 \
\cb3         \cf2 return\cf4  embed\cb1 \
\
\cb3     \cf6 def\cf4  \cf9 get_data_generator\cf4 (\cf10 self\cf4 , \cf10 shear_range\cf4 =\cf11 0.4\cf4 , \cf10 zoom_range\cf4 =\cf11 0.4\cf4 , \cf10 horizontal_flip\cf4 =\cf6 True\cf4 , \cf10 rotation_range\cf4 =\cf11 40\cf4 ):\cb1 \
\cb3         \cf8 '''\cf4 \cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf8 \cb3         data generator\cf4 \cb1 \
\cf8 \cb3         '''\cf4 \cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb3         \cf2 return\cf4  ImageDataGenerator(\cb1 \
\cb3             \cf10 shear_range\cf4 =shear_range,\cb1 \
\cb3             \cf10 zoom_range\cf4 =zoom_range,\cb1 \
\cb3             \cf10 horizontal_flip\cf4 =horizontal_flip,\cb1 \
\cb3             \cf10 rotation_range\cf4 =rotation_range)\cb1 \
\cb3     \cb1 \
\cb3     \cf6 def\cf4  \cf9 generate_next_batch\cf4 (\cf10 self\cf4 , \cf10 generator\cf4 , \cf10 training_data\cf4 , \cf10 batch_size\cf4 ):\cb1 \
\cb3         \cf5 # batch_size = 20\cf4 \cb1 \
\cb3         \cf2 for\cf4  batch \cf6 in\cf4  generator.flow(training_data, \cf10 batch_size\cf4 =batch_size):\cb1 \
\cb3             gray = gray2rgb(rgb2gray(batch))\cb1 \
\cb3             embed = \cf6 self\cf4 .create_inception_embedding(gray)\cb1 \
\cb3             lab_batch = rgb2lab(batch)\cb1 \
\cb3             x_batch = lab_batch[:,:,:,\cf11 0\cf4 ]\cb1 \
\cb3             x_batch = x_batch.reshape(x_batch.shape+(\cf11 1\cf4 ,))\cb1 \
\cb3             y_batch = lab_batch[:,:,:,\cf11 1\cf4 :] / \cf11 128\cf4 \cb1 \
\cb3             \cf2 yield\cf4  ([x_batch, embed, y_batch])\cb1 \
\
\cb3     \cf6 def\cf4  \cf9 train\cf4 (\cf10 self\cf4 , \cf10 batch_size\cf4 , \cf10 epochs\cf4 , \cf10 training_data\cf4 , \cf10 steps\cf4 ):\cb1 \
\cb3         \cf8 '''\cf4 \cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf8 \cb3         Train Model\cf4 \cb1 \
\cf8 \cb3         '''\cf4 \cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb3         generator = \cf6 self\cf4 .get_data_generator()\cb1 \
\cb3         \cf6 self\cf4 .model.compile(\cf10 optimizer\cf4 =\cf8 'adam'\cf4 , \cf10 loss\cf4 =\cf8 'mse'\cf4 )\cb1 \
\cb3         \cf6 self\cf4 .model.fit_generator(\cf6 self\cf4 .generate_next_batch(generator, training_data, batch_size), \cf10 epochs\cf4 =epochs, \cf10 steps_per_epoch\cf4 =steps)\cb1 \
\
\cb3     \cf6 def\cf4  \cf9 predict\cf4 (\cf10 self\cf4 , \cf10 validation_data\cf4 , \cf10 category\cf4 ):\cb1 \
\cb3         \cf8 '''\cf4 \cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf8 \cb3         Make predictions\cf4 \cb1 \
\cf8 \cb3         '''\cf4 \cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb3         test = gray2rgb(rgb2gray(validation_data))\cb1 \
\cb3         test_embed = \cf6 self\cf4 .create_inception_embedding(validation_data)\cb1 \
\cb3         test = rgb2lab(test)[:,:,:,\cf11 0\cf4 ]\cb1 \
\cb3         test = test.reshape(test.shape+(\cf11 1\cf4 ,))\cb1 \
\
\cb3         \cf5 # test\cf4 \cb1 \
\cb3         output = \cf6 self\cf4 .model.predict([test, test_embed])\cb1 \
\cb3         output = output * \cf11 128\cf4 \cb1 \
\
\cb3         \cf2 for\cf4  i \cf6 in\cf4  \cf9 range\cf4 (\cf9 len\cf4 (output)):\cb1 \
\cb3             cur = np.zeros((\cf11 256\cf4 , \cf11 256\cf4 , \cf11 3\cf4 ))\cb1 \
\cb3             cur[:,:,\cf11 0\cf4 ] = test[i][:,:,\cf11 0\cf4 ]\cb1 \
\cb3             cur[:,:,\cf11 1\cf4 :] = output[i]\cb1 \
\
\cb3             imsave(\cf8 "./testset/"\cf4  + category + \cf8 "/predicted/"\cf4  + image_names[i], lab2rgb(img))\cb1 \
\
\
\
\pard\pardeftab720\sl360\partightenfactor0
\cf6 \cb3 def\cf4  \cf9 get_image_data\cf4 (\cf10 category\cf4 , \cf10 split\cf4 =\cf11 0.8\cf4 ):\cb1 \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb3     data = []\cb1 \
\cb3     all_images = os.listdir(\cf8 "./adjusted/"\cf4  + category)\cb1 \
\cb3     \cf2 for\cf4  name \cf6 in\cf4  all_images:\cb1 \
\cb3         data.append(imread(\cf8 "./adjusted/"\cf4  + category + \cf8 "/"\cf4  + name))\cb1 \
\cb3     data = np.array(data, \cf10 dtype\cf4 =\cf7 float\cf4 )\cb1 \
\cb3     training_data = data[\cf7 int\cf4 ((\cf11 1\cf4  - split) * \cf9 len\cf4 (data)):]\cb1 \
\cb3     training_data = \cf11 1.0\cf4 /\cf11 255\cf4  * training_data\cb1 \
\cb3     validation_data = data[:\cf7 int\cf4 ((\cf11 1\cf4  - split) * \cf9 len\cf4 (data))]\cb1 \
\cb3     validation_data = \cf11 1.0\cf4 /\cf11 255\cf4  * validation_data\cb1 \
\cb3     \cf2 return\cf4  (training_data, validation_data)\cb1 \
}